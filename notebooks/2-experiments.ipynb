{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered 0.21 of train data\n"
     ]
    }
   ],
   "source": [
    "# директория с данными\n",
    "DATA_DIR = \"../data\"\n",
    "# минимальное количество просмотров для трейна\n",
    "MIN_VIEWS_COUNT = 20\n",
    "# клип для отношения рейтинга к просмотрам, (None, None), если клип не нужен\n",
    "CLIP_RATING_BY_VIEWS = (0, 1)\n",
    "\n",
    "train_data = pd.read_json(f\"{DATA_DIR}/poetry_data_train.json\")\n",
    "\n",
    "# отбрасываем объекты с маленьким количеством просмотров\n",
    "print(f\"filtered {round((train_data['views'] < MIN_VIEWS_COUNT).mean(), 2)} of train data\")\n",
    "train_data = train_data[train_data[\"views\"] >= MIN_VIEWS_COUNT]\n",
    "\n",
    "test_data = pd.read_json(f\"{DATA_DIR}/poetry_data_test.json\")\n",
    "\n",
    "# считаем отношение рейтинга к просмотрам \n",
    "# можно будет попробовать это в экспериментах\n",
    "for df in train_data, test_data:\n",
    "    df[\"rating_by_views\"] = (df[\"rating\"] / df[\"views\"]).clip(*CLIP_RATING_BY_VIEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'distribution of `rating_by_views`'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data[\"rating_by_views\"].plot(kind=\"hist\", bins=100, grid=True, title=\"distribution of `rating_by_views`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.retrieval import RetrievalNormalizedDCG, RetrievalPrecision\n",
    "from torchmetrics.regression import MeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Sampler\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            tokenizer: AutoTokenizer,\n",
    "            max_length: int = 512,\n",
    "            target_expr: str | None = None,\n",
    "            extra_keys: tuple[str, ...] = tuple(),\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "        self._tokenizer = tokenizer\n",
    "        self._target_expr = target_expr\n",
    "        self._extra_keys = extra_keys\n",
    "        self._max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        row = self._df.iloc[idx]\n",
    "\n",
    "        text = row[\"output_text\"]\n",
    "        tokenizer_out = self._tokenizer(text, padding=\"max_length\", truncation=True, max_length=self._max_length, return_tensors='pt', )\n",
    "\n",
    "        if self._target_expr:\n",
    "            target = eval(self._target_expr, {**row.to_dict(), \"np\": np})\n",
    "        else:\n",
    "            target = row[\"rating\"]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokenizer_out[\"input_ids\"],\n",
    "            \"attention_mask\": tokenizer_out[\"attention_mask\"],\n",
    "            \"target\": torch.tensor(target, dtype=torch.float32),\n",
    "            **{\n",
    "                key: row[key] for key in self._extra_keys\n",
    "            }\n",
    "        }\n",
    "\n",
    "class RegressionDataModule(lightning.LightningDataModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: RegressionDataset,\n",
    "        val_size: float = 0.2,\n",
    "        batch_size: int = 8,\n",
    "        sampler_type: str | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        val_size = int(len(dataset) * val_size)\n",
    "        train_size = len(dataset) - val_size\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # 1 вариант: берем нулевые объекты с меньшим весом\n",
    "        if sampler_type == \"zero_ratings\":\n",
    "            sampler = WeightedRandomSampler(\n",
    "                np.where(self.train_dataset.dataset._df[\"rating\"].iloc[self.train_dataset.indices].values == 0, 0.1, 0.9),\n",
    "                num_samples=train_size,\n",
    "                replacement=True,\n",
    "            )\n",
    "        elif sampler_type == \"new2\":\n",
    "            train_data = self.train_dataset.dataset._df.iloc[self.train_dataset.indices, :]\n",
    "    \n",
    "            rating_weights = train_data[\"rating\"].rank(method=\"dense\") / train_data[\"rating\"].rank(method=\"dense\").max()\n",
    "            views_weights = train_data[\"views\"].rank(method=\"dense\") / train_data[\"views\"].rank(method=\"dense\").max()\n",
    "\n",
    "            union_weights = (0.4 * views_weights + 0.6 * rating_weights).values\n",
    "            sampler = WeightedRandomSampler(\n",
    "                union_weights,\n",
    "                num_samples=train_size,\n",
    "                replacement=True,\n",
    "            )\n",
    "        else:\n",
    "            sampler = None\n",
    "        \n",
    "        self.dataloader_kwargs = {\"shuffle\": True}\n",
    "        if sampler is not None:\n",
    "            del self.dataloader_kwargs[\"shuffle\"]\n",
    "            self.dataloader_kwargs[\"sampler\"] = sampler\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, **self.dataloader_kwargs, )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size,)\n",
    "    \n",
    "    def transfer_batch_to_device(self, batch, device, dataloader_idx):\n",
    "        extra_batch = {}\n",
    "        for key in self.train_dataset.dataset._extra_keys:\n",
    "            extra_batch[key] = batch.pop(key)\n",
    "        extra_batch.update(super().transfer_batch_to_device(batch, device, dataloader_idx))\n",
    "        return extra_batch\n",
    "\n",
    "\n",
    "class RegressionModule(lightning.LightningModule):\n",
    "\n",
    "    def __init__(self, model: AutoModel, n_epochs: int, freeze: bool = True, lr: float = 0.0001):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.model = model\n",
    "        if freeze:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 256),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "        self.ndcg = RetrievalNormalizedDCG()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.n_epochs)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = self.fc(x.pooler_output)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch: dict, **_):\n",
    "  \n",
    "        input_ids = batch[\"input_ids\"].reshape(batch[\"input_ids\"].shape[0], batch[\"input_ids\"].shape[-1])\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        out = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.functional.mse_loss(out.squeeze(), batch[\"target\"].squeeze())\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        ndcg_score = self.ndcg(out.squeeze(), batch[\"target\"].squeeze(), indexes=torch.from_numpy(LabelEncoder().fit_transform(batch[\"genre\"])))\n",
    "        self.log(\"train_ndcg\", ndcg_score, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: dict, **_):\n",
    "                \n",
    "        input_ids = batch[\"input_ids\"].reshape(batch[\"input_ids\"].shape[0], batch[\"input_ids\"].shape[-1])\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        out = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.functional.mse_loss(out.squeeze(), batch[\"target\"].squeeze())\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        ndcg_score = self.ndcg(out, batch[\"target\"], indexes=LabelEncoder().transform(batch[\"genre\"]))\n",
    "        self.log(\"val_ndcg\", ndcg_score, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "n_epochs = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/LaBSE-en-ru\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/LaBSE-en-ru\")\n",
    "model.train()\n",
    "\n",
    "dataset = RegressionDataset(df=train_data, tokenizer=tokenizer, target_expr=\"rating / views\", extra_keys=(\"genre\", ))\n",
    "\n",
    "datamodule = RegressionDataModule(dataset=dataset, batch_size=32, val_size=0.1, sampler_type=\"new2\")\n",
    "module = RegressionModule(model=model, n_epochs=n_epochs, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                   | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model | BertModel              | 128 M  | train\n",
      "1 | fc    | Sequential             | 197 K  | train\n",
      "2 | ndcg  | RetrievalNormalizedDCG | 0      | train\n",
      "---------------------------------------------------------\n",
      "197 K     Trainable params\n",
      "128 M     Non-trainable params\n",
      "128 M     Total params\n",
      "514.171   Total estimated model params size (MB)\n",
      "234       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/vyacheslav.kostrov/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/vyacheslav.kostrov/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce84fc259449487cad00eacb3cd09313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/optim/adamw.py:165\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 165\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[102], line 145\u001b[0m, in \u001b[0;36mRegressionModule.training_step\u001b[0;34m(self, batch, **_)\u001b[0m\n\u001b[1;32m    143\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 145\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(out\u001b[38;5;241m.\u001b[39msqueeze(), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[102], line 136\u001b[0m, in \u001b[0;36mRegressionModule.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m--> 136\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x\u001b[38;5;241m.\u001b[39mpooler_output)\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    584\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    438\u001b[0m )\n\u001b[0;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m lightning\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlightning\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mloggers\u001b[38;5;241m.\u001b[39mTensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mn_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     num_sanity_val_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = lightning.Trainer(\n",
    "    logger=lightning.pytorch.loggers.TensorBoardLogger(\"./\"),\n",
    "    max_epochs=n_epochs,\n",
    "    val_check_interval=500,\n",
    "    check_val_every_n_epoch=1,\n",
    "    log_every_n_steps=10,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.fit(model=module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "onegin_sample = \"\"\"Мой дядя самых честных правил,\n",
    "Когда не в шутку занемог,\n",
    "Он уважать себя заставил\n",
    "И лучше выдумать не мог.\n",
    "Его пример другим наука;\n",
    "Но, боже мой, какая скука\n",
    "С больным сидеть и день и ночь,\n",
    "Не отходя ни шагу прочь!\n",
    "Какое низкое коварство\n",
    "Полуживого забавлять,\n",
    "Ему подушки поправлять,\n",
    "Печально подносить лекарство,\n",
    "Вздыхать и думать про себя:\n",
    "Когда же черт возьмет тебя!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2373]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.eval()\n",
    "tokenized_onegin_sample = tokenizer(onegin_sample, padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n",
    "model_score = module(input_ids=tokenized_onegin_sample.input_ids, attention_mask=tokenized_onegin_sample.attention_mask)\n",
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_samples = test_data.query(\"rating < 5 and views > 5000\").output_text.values\n",
    "\n",
    "# for bad_sample in bad_samples:\n",
    "#     tokenized_bad_sample = tokenizer(bad_sample, padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n",
    "#     model_score = module(input_ids=tokenized_bad_sample.input_ids, attention_mask=tokenized_bad_sample.attention_mask)\n",
    "#     print(model_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for good_sample in test_data.query(\"rating > 400 and views > 1000\").output_text.values:\n",
    "#     tokenized_good_sample = tokenizer(good_sample, padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n",
    "#     model_score = module(input_ids=tokenized_good_sample.input_ids, attention_mask=tokenized_good_sample.attention_mask)\n",
    "#     print(model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Sampler\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            tokenizer: AutoTokenizer,\n",
    "            max_length: int = 512,\n",
    "            target_expr: str | None = None,\n",
    "            extra_keys: tuple[str, ...] = tuple(),\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "        self._tokenizer = tokenizer\n",
    "        self._target_expr = target_expr\n",
    "        self._extra_keys = extra_keys\n",
    "        self._max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        row = self._df.iloc[idx]\n",
    "\n",
    "        text = row[\"output_text\"]\n",
    "        tokenizer_out = self._tokenizer(text, padding=\"max_length\", truncation=True, max_length=self._max_length, return_tensors='pt', )\n",
    "\n",
    "        if self._target_expr:\n",
    "            target = eval(self._target_expr, {**row.to_dict(), \"np\": np})\n",
    "        else:\n",
    "            target = row[\"rating\"] > 0\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokenizer_out[\"input_ids\"],\n",
    "            \"attention_mask\": tokenizer_out[\"attention_mask\"],\n",
    "            \"target\": torch.tensor(target, dtype=torch.long),\n",
    "            **{\n",
    "                key: row[key] for key in self._extra_keys\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ClassificationDataModule(lightning.LightningDataModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: ClassificationDataset,\n",
    "        val_size: float = 0.2,\n",
    "        batch_size: int = 8,\n",
    "        sampler_type: str | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        val_size = int(len(dataset) * val_size)\n",
    "        train_size = len(dataset) - val_size\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # 1 вариант: берем нулевые объекты с меньшим весом\n",
    "        if sampler_type == \"zero_ratings\":\n",
    "            sampler = WeightedRandomSampler(\n",
    "                np.where(self.train_dataset.dataset._df[\"rating\"].iloc[self.train_dataset.indices].values == 0, 0.1, 0.9),\n",
    "                num_samples=train_size,\n",
    "                replacement=True,\n",
    "            )\n",
    "        elif sampler_type == \"new2\":\n",
    "            train_data = self.train_dataset.dataset._df.iloc[self.train_dataset.indices, :]\n",
    "    \n",
    "            rating_weights = train_data[\"rating\"].rank(method=\"dense\") / train_data[\"rating\"].rank(method=\"dense\").max()\n",
    "            views_weights = train_data[\"views\"].rank(method=\"dense\") / train_data[\"views\"].rank(method=\"dense\").max()\n",
    "\n",
    "            union_weights = (0.4 * views_weights + 0.6 * rating_weights)\n",
    "            sampler = WeightedRandomSampler(\n",
    "                union_weights,\n",
    "                num_samples=train_size,\n",
    "                replacement=True,\n",
    "            )\n",
    "        else:\n",
    "            sampler = None\n",
    "        \n",
    "        self.dataloader_kwargs = {\"shuffle\": True}\n",
    "        if sampler is not None:\n",
    "            del self.dataloader_kwargs[\"shuffle\"]\n",
    "            self.dataloader_kwargs[\"sampler\"] = sampler\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, **self.dataloader_kwargs, )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size,)\n",
    "\n",
    "\n",
    "class ClassificationModule(lightning.LightningModule):\n",
    "\n",
    "    def __init__(self, model: AutoModel, n_epochs: int, freeze: bool = True, lr: float = 0.0001):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.model = model\n",
    "        if freeze:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(312, 256),\n",
    "            torch.nn.Sigmoid(),\n",
    "            # torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.Linear(256, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.n_epochs)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = self.fc(x.pooler_output)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch: dict, **_):\n",
    "  \n",
    "        input_ids = batch[\"input_ids\"].reshape(batch[\"input_ids\"].shape[0], batch[\"input_ids\"].shape[-1])\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        out = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.BCELoss()(out.squeeze(), batch[\"target\"].float())\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: dict, **_):\n",
    "                \n",
    "        input_ids = batch[\"input_ids\"].reshape(batch[\"input_ids\"].shape[0], batch[\"input_ids\"].shape[-1])\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        out = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.BCELoss()(out.squeeze(), batch[\"target\"].float())\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "n_epochs = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "dataset = ClassificationDataset(df=train_data, tokenizer=tokenizer, target_expr=\"rating > 3\",)\n",
    "\n",
    "datamodule = ClassificationDataModule(dataset=dataset, batch_size=32, val_size=0.1, sampler_type=None)\n",
    "module = ClassificationModule(model=model, n_epochs=n_epochs, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.rating.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | BertModel  | 11.8 M | eval \n",
      "1 | fc    | Sequential | 80.4 K | train\n",
      "---------------------------------------------\n",
      "80.4 K    Trainable params\n",
      "11.8 M    Non-trainable params\n",
      "11.9 M    Total params\n",
      "47.458    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "66        Modules in eval mode\n",
      "/Users/vyacheslav.kostrov/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/vyacheslav.kostrov/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8def245c7b604535ac12b0e631f0459b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa27fa411374547bec727ee0ef8edee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c84f8f001f7448dbae2ff43eb4164f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14fbe111b8244df8e8510ab9971666c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34adaeda158949678e029305d816d06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9554b57f9004f3293cf855123460749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341ca6ffcb7240e6b9dbac450e810acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead57aed579f4ba4a367989547e2793d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03187d35ac5f4c15b897f488d4389dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d07ce341e244e199955e97b9ee4ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4885227bbdd24ec7953795e106cfa842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5e207716404feea9ce18291f917c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:269\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m--> 269\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_batch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_output, batch, batch_idx)\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:218\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/callbacks/progress/tqdm_progress.py:279\u001b[0m, in \u001b[0;36mTQDMProgressBar.on_train_batch_end\u001b[0;34m(self, trainer, pl_module, outputs, batch, batch_idx)\u001b[0m\n\u001b[1;32m    278\u001b[0m _update_n(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_progress_bar, n)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/callbacks/progress/progress_bar.py:198\u001b[0m, in \u001b[0;36mProgressBar.get_metrics\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    197\u001b[0m standard_metrics \u001b[38;5;241m=\u001b[39m get_standard_metrics(trainer)\n\u001b[0;32m--> 198\u001b[0m pbar_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_bar_metrics\u001b[49m\n\u001b[1;32m    199\u001b[0m duplicates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(standard_metrics\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m&\u001b[39m pbar_metrics\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1635\u001b[0m, in \u001b[0;36mTrainer.progress_bar_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The metrics sent to the progress bar.\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \n\u001b[1;32m   1631\u001b[0m \u001b[38;5;124;03mThis includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\u001b[39;00m\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;124;03m:paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m \n\u001b[1;32m   1634\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logger_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_bar_metrics\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:253\u001b[0m, in \u001b[0;36m_LoggerConnector.progress_bar_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_results:\n\u001b[0;32m--> 253\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpbar\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_bar_metrics\u001b[38;5;241m.\u001b[39mupdate(metrics)\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:234\u001b[0m, in \u001b[0;36m_LoggerConnector.metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mon_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:490\u001b[0m, in \u001b[0;36m_ResultCollection.metrics\u001b[0;34m(self, on_step)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result_metric\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mprog_bar:\n\u001b[0;32m--> 490\u001b[0m         metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpbar\u001b[39m\u001b[38;5;124m\"\u001b[39m][forked_name] \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_tensors_to_scalars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/fabric/utilities/apply_func.py:136\u001b[0m, in \u001b[0;36mconvert_tensors_to_scalars\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_item\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py:64\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype):  \u001b[38;5;66;03m# single element\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/fabric/utilities/apply_func.py:134\u001b[0m, in \u001b[0;36mconvert_tensors_to_scalars.<locals>.to_item\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe metric `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` does not contain a single element, thus it cannot be converted to a scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m lightning\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlightning\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mloggers\u001b[38;5;241m.\u001b[39mTensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mn_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     num_sanity_val_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = lightning.Trainer(\n",
    "    logger=lightning.pytorch.loggers.TensorBoardLogger(\"./\"),\n",
    "    max_epochs=n_epochs,\n",
    "    val_check_interval=500,\n",
    "    check_val_every_n_epoch=1,\n",
    "    log_every_n_steps=10,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.fit(model=module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bi encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vyacheslav.kostrov/my/hse-mlds/subjects/deep-learning-1/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 109556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3d3774bc4a429db830e9272858b676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3424 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "\n",
    "bi_encoder = SentenceTransformer('distiluse-base-multilingual-cased-v1', device=\"mps\")  #('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256  \n",
    "top_k = 32                          \n",
    "\n",
    "cross_encoder = CrossEncoder('DiTy/cross-encoder-russian-msmarco', device=\"mps\")\n",
    "\n",
    "passages = train_data.output_text.values.tolist()\n",
    "print(\"Passages:\", len(passages))\n",
    "\n",
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb29ccc14cd048fe8ce563bf5dd2b9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109556 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(passages):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will search all wikipedia articles for passages that\n",
    "# answer the query\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    print(\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    ##### Semantic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.to(\"mps\") #.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: Выведи лучшее стихотворение\n",
      "Top-3 lexical search (BM25) hits\n",
      "\t13.605\tСяду в автобус, в любом направлении, От суеты в тишину убегу - Вдруг да напишется стихотворение, Там, где околицы тонут в снегу!  Там после сна предрассветного, длинного, Радуя новым восходом страну, Солнце морозное встало малиново, Чтобы окрасить снегов белизну.  И, поднимая моё настроение, Сердцу быстрее забиться велит! Так вот и пишется стихотворение - Буква за буквой, и весь алфавит!  Я, напрягая уставшее зрение, Стану с собой, молодым, наравне... Так вот и пишется стихотворение - Дымом по ветру, морозом в окне.  Сразу забуду про возраст свой древний я, Буду любовь воспевать и добро! Так вот и пишется стихотворение - Инеем в бороду,\n",
      "\t13.371\tЦветёт сирень неистово Пахучая, прохладная. С ветвей росинки чистые В ладонь мне тихо падают. Я в день иду сиреневый, Иду не настороженно, Ищу стихотворение - Весеннее, хорошее. Моё стихотворение, Оно пока не встречено, Но этим днём сиреневым До жилочек просвечено.\n",
      "\t12.685\tСамое короткое стихотворение Ольга Сигаль Ты мне - не нужен Я тебя - ЛЮБЛЮ!\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Bi-Encoder Retrieval hits\n",
      "\t0.569\tЯ пройдусь по любимым стихам, Проглотив их огромным глотком. Строчки... буквы из них не отдам, Наслаждаясь как сочным плодом.  Капли, в рифмы стремглав прорастут, Оживая в любовный сонет. Ты мой самый любимый грейпфрут, Ты мой самый любимый поэт!\n",
      "\t0.554\tСамый любимый стих - это тот, который, Если его не окажется в интернете, Ты набираешь по памяти - все сто сорок Строк - чтоб отправить девчонке - одной на свете.  И если, его прочтя, она не ответит, Что у нее от стиха тоже сносит крышу, В ленте утопишь страницу ее, как в Лете. Жаль, что стихов таких больше почти не пишут.\n",
      "\t0.552\tЭх, поэзия влечёт. Сильнейшая привычка Так затянет, засосёт. Зажжёт, не надо спичек\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "\t0.947\tСамый любимый стих - это тот, который, Если его не окажется в интернете, Ты набираешь по памяти - все сто сорок Строк - чтоб отправить девчонке - одной на свете.  И если, его прочтя, она не ответит, Что у нее от стиха тоже сносит крышу, В ленте утопишь страницу ее, как в Лете. Жаль, что стихов таких больше почти не пишут.\n",
      "\t0.567\tСтихи любимого поэта Сроднили души и сердца. В них столько жизненного света И слов прекрасных отТворца.\n",
      "\t0.398\tХорошие стихи - Они нужны для счастья, Решения проблем, И щедрой красоты. Не будет день плохим, Пусть даже и в ненастье, Присутствие дилемм Слабее доброты.  Всего лишь пара строк И в норме настроение, Улыбки и цветы - Гармония души. Уютный уголок, Мечты и утешения, А жизнь с тобой на ты, Пиши поэт! Пиши!\n"
     ]
    }
   ],
   "source": [
    "search(query = \"Выведи лучшее стихотворение\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: Стихотворение про осень\n",
      "Top-3 lexical search (BM25) hits\n",
      "\t16.485\tСяду в автобус, в любом направлении, От суеты в тишину убегу - Вдруг да напишется стихотворение, Там, где околицы тонут в снегу!  Там после сна предрассветного, длинного, Радуя новым восходом страну, Солнце морозное встало малиново, Чтобы окрасить снегов белизну.  И, поднимая моё настроение, Сердцу быстрее забиться велит! Так вот и пишется стихотворение - Буква за буквой, и весь алфавит!  Я, напрягая уставшее зрение, Стану с собой, молодым, наравне... Так вот и пишется стихотворение - Дымом по ветру, морозом в окне.  Сразу забуду про возраст свой древний я, Буду любовь воспевать и добро! Так вот и пишется стихотворение - Инеем в бороду,\n",
      "\t14.155\tХочешь, подарю настроение Сочиню для тебя стихотворение Не знаю, правда, пока про что Но точно будет оно с добром Наивно, смешно, пускай порой Зато от души, и сердца строкой  Читая его, улыбнёшься Настроения наберёшься Этого я добиваюсь Печали твои прогоняя\n",
      "\t13.416\tПро осень... Про осень... Про осень... Написаны песни и строчки... Про клёны, про ивы, про просинь, Про пёстрых осинок сорочки, Про тополь, про липы, рябины, Про ясень, про дождика слёзы, Про грусть расставанья с любимым, А я, как всегда, про берёзы... Берёзы... Берёзы... Берёзы... Прекрасны весной, зимой, летом. Люблю их в жару и в морозы, Ревную к закатам, к рассветам. Ревную их к солнцу и ветру, К метелям зимой, летом к грозам. Сегодня скажу по секрету, Я осень ревную к берёзам... Ах осень... Ах осень... Ах осень... Берёзки нарядит, разденет... Про чувства мои и не спросит До боли любимое время. Я с осенью каждой взрослее, Чем старше, тем больше любви, Пусть осень меня не жалеет, Лишь любит берёзки мои.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Bi-Encoder Retrieval hits\n",
      "\t0.617\tХорошие стихи ласкают душу. От слёз поэтов солона Земля. Быть может от того весной и летом Цветами покрывается она.\n",
      "\t0.612\tСтихи осеннего разлива... В них ласки свет, И впечатленье - ярко, живо - Любви то след.  Стихи осеннего разлива... Их пригуби! Дыханье дней моих счастливых Не погуби!\n",
      "\t0.601\tБанальным быть я не хочу, Хотя порой и подмывает. Стихи про осень не строчу, Любой поэт их сочиняет. Что можно нового сказать? Всё досконально описали. Осталось только повторять, О ветре, листьях и печали. Бессмертна тема о дождях, Про журавлей не забывают. Хватает строк о сентябрях, Сезон осенний восхваляют. Что скоро грянут холода, Как нас не радует погода. И льются строки, как вода, Такое видно время года. Меня простите, что ворчу, Со мной редко так бывает. Писать про осень не хочу, Сейчас любой их сочиняет.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "\t0.969\tПро осень... Про осень... Про осень... Написаны песни и строчки... Про клёны, про ивы, про просинь, Про пёстрых осинок сорочки, Про тополь, про липы, рябины, Про ясень, про дождика слёзы, Про грусть расставанья с любимым, А я, как всегда, про берёзы... Берёзы... Берёзы... Берёзы... Прекрасны весной, зимой, летом. Люблю их в жару и в морозы, Ревную к закатам, к рассветам. Ревную их к солнцу и ветру, К метелям зимой, летом к грозам. Сегодня скажу по секрету, Я осень ревную к берёзам... Ах осень... Ах осень... Ах осень... Берёзки нарядит, разденет... Про чувства мои и не спросит До боли любимое время. Я с осенью каждой взрослее, Чем старше, тем больше любви, Пусть осень меня не жалеет, Лишь любит берёзки мои.\n",
      "\t0.967\tСтихи осеннего разлива... В них ласки свет, И впечатленье - ярко, живо - Любви то след.  Стихи осеннего разлива... Их пригуби! Дыханье дней моих счастливых Не погуби!\n",
      "\t0.939\tВ мои стихи осенние Влетает птичий клин, Пророча запустение И в серых брызгах сплин.  По строчкам расстилается Закатных слов туман, И до весны прощается Наш с осенью роман.  Снежинок первых кружево Зовёт листву в полёт, И в звуках ветра вьюжного Всё, кажется, умрёт.  Но в золоте купается Берёзовая рать. Листком к листку слагается Октябрьская тетрадь.\n"
     ]
    }
   ],
   "source": [
    "search(query = \"Стихотворение про осень\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: попробвать как-то адаптировать, \"из коробки\" скорее поиск по тематике получается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
